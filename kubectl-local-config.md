## Configuring kubectl locally to connect to your cluster

If you want to use kubectl locally (from your machine, or any other instance than the ones we created for the cluster), you need to create a `config` file and place it in your `$HOME/.kube/` dir. 

`kubectl` uses the `$HOME/.kube/config` to get all the necessary info and certificates to connect to your cluster.

### 1. Regenerate the certificate and admin.conf file

If you log in the `controller-0` node, you fill find a file called `admin.conf` under `/etc/kubernetes`. That file is generate automatically by `kubeadm` at the moment of the creation of the cluster and contains all the necessary config info to connect to the K8S API Server.

The **problem**, is that the certificate contained in that file (generated by `kubeadm`) is **only valid for the internal address of the controller node**. 

So we need to regenerate the certificate to include also the external (public) IP address of the `controller-0` node.

So...

#### Pull the kubeadm configuration into a file
First we need to retrieve the cluster configuration. It is stored as a `configmap` in the `kube-system` namespace, so we can get it by:
```
kubectl -n kube-system get configmap kubeadm-config -o jsonpath='{.data.ClusterConfiguration}' > kubeadm.yaml
```
That will create a local yaml file that contains the cluster configuration. 

You'll obtain a file that looks like this: 
```
apiServer:
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
apiVersion: kubeadm.k8s.io/v1beta2
certificatesDir: /etc/kubernetes/pki
clusterName: kubernetes
controllerManager: {}
dns:
  type: CoreDNS
etcd:
  local:
    dataDir: /var/lib/etcd
imageRepository: k8s.gcr.io
kind: ClusterConfiguration
kubernetesVersion: v1.18.4
networking:
  dnsDomain: cluster.local
  podSubnet: 192.168.0.0/16
  serviceSubnet: 10.96.0.0/12
scheduler: {}
```

#### Add a cert SAN
Now you want to add the external IP address of your cluster's controller node `controller-0`. 

Retrieve that IP address: 
```
gcloud compute instances list --filter=name=controller-0 --format="value(EXTERNAL_IP)"
```

and now add that address in the yaml file. Under `apiServer`, create (if it doesn't exist) a `certSANs` list: 
```
apiServer:
  certSANs:
  - "<ip address>"
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
...
```
for example (**mind the QUOTES!**): 
```
apiServer:
  certSANs:
  - "34.76.110.157"
  extraArgs:
    authorization-mode: Node,RBAC
  timeoutForControlPlane: 4m0s
...
```
This change to the kubeadm configuration file adds SANs for the IP address `34.76.110.157`. This would be in addition to the standard list of SANs that are included (which would be the local hostname, some names for the default Kubernetes Service object, the default IP address for the Kubernetes Service object, and the primary IP address of the node).

#### Regenerate the certificate

Now move the existing API server certificate and key out of the `/etc/kubernetes/pki` directory, because **if kubeadm sees that they already exist it wonâ€™t create new ones**!. 
```
sudo mv /etc/kubernetes/pki/apiserver.{crt,key} ~
```

Then, use kubeadm to generate a new certificate.

```
sudo kubeadm init phase certs apiserver --config kubeadm.yaml
```

### 2. Configure your local environment
#### Copy the new admin.conf

Now the new `/etc/kubernetes/admin.conf` file will contain an updated certificate. 

#### Create the local config file

So go back to your local machine and download that file locally: 

```
gcloud compute scp root@controller-0:/etc/kubernetes/admin.conf .
```

Now you will have a local file called `admin.conf` that should look like this: 
```
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUN5RENDQWJDZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01EWXlNVEEzTURVd01Gb1hEVE13TURZeE9UQTNNRFV3TUZvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBT2drClFUMWxYNXRhdkZZZDZFNkZTMlp2MEplQllib0RBSnk5bkROY25UK1Z1MXNiWmgzTFl6UFZuNlNrM05yR0Y2ZEkKRWVUdkJjd3dtY0g5NkRpV1hrcFlWejVOdlE5ZWkrTUVOVWNSaEdId09FQTJBVWZuclpJTVFHZHpWTU5zaHFIVQpOTWo2NENaZURrbDJEcldneERWSVZhYWV2ancxTmlrOWpnbFJSZnB4blpCeDR0bGJkc0kxV3J5aWV5ZHFDMmczCk55SXcrYklSQ2xzU0VWTHR3ZGluSTRVSGpaenBCMXVESnNSMUtwdTBiNzlGQ1FrMkhrdW8wMytCdDJtbDZaNWgKZy92UnB3RlNHN05vNEVwZUFmQnBzVGorWXRPQUFYTE9KRm53Sk5xL25tdVpqNHZSaaaaS0tLQo=
    server: https://10.240.0.11:6443
  name: kubernetes
contexts:
- context:
... 
```

As you can see, the server IP address is referring to the local subnet address of the `controller-0` node so you need to change that and replace that IP address (in the example `10.240.0.11`) with the external IP assigned by GCE to the compute instance of `controller-0`. 

Now you just need to copy the updated `admin.conf` file to your `~/.kube/` directory: 

```
cp admin.conf ~/.kube/config
```

#### Test it!

Now if you run `kubectl get nodes` you should retrieve the list of nodes of your cluster! 
```
controller-0   Ready    master   71m   v1.18.4
worker-0       Ready    <none>   69m   v1.18.4
worker-1       Ready    <none>   69m   v1.18.4
```

You're done! 

Note that here we're using the **ephemeral external IP address** that Google provides when creating compute instances. That is **highly inefficient**, because every time you will stop and restart a compute instance that address will change, so if you plan on using `kubectl` locally a lot, you might want to **assign a reserved (static) external IP address** to your `controller-0` instance, otherwise you'll have to repeat these steps every time you restart your nodes. 

