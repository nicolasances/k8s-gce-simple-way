## Step 3 : Creating an NGINX Load Balancer

So again, why am I doing this? 

Well, if `--cloud-provider` is enabled (with the `cloud-config` correctly set), when creating a Service or Ingress of type `LoadBalancer`, K8S will spin off automatically a Google Cloud Load Balancer. <br>
... and that stuff is expensive (from my perspective and for my goal, at least).

I would be fine with just a simple NGINX HTTP Load Balancer put in front of my cluster and redirecting the traffic the the K8S nodes where the Ingress is going to pick it up. 

So we're going to create a Google Compute Instance, install NGINX and route the traffic to the K8S nodes. 

That helps also because right now (after Step 2) the Ingress exposing my services to the outside world are exposed on a really annoying `node port` and who likes connecting to a weird port? I would prefer connecting on port 80 and have a simple `http://api.my.domain/...` call... 

So.. 

### 3.1. Create a new Compute Instance

Let's create a compute instance called `proxy-0` that is just going to accept API calls on port 80 and forward that to the K8S Ingress that we created before:

```
gcloud compute instances create proxy-0 \
    --async \
    --boot-disk-size 20GB \
    --can-ip-forward \
    --image=centos-8-v20200618 
    --image-project=centos-cloud 
    --machine-type=f1-micro \
    --private-network-ip 10.240.0.31 \
    --scopes compute-rw,storage-ro,service-management,service-control,logging-write,monitoring \
    --subnet ksub \
    --zone europe-west1-c \
    --tags kube,controller
```

Good, let's go on with installing NGINX...

### 3.2. Install NGINX

To install NGINX, you can do the following (source: [Install NGINX on CentOS](https://www.digitalocean.com/community/tutorials/how-to-install-nginx-on-centos-7)): 

```
{
sudo yum install epel-release
sudo yum install nginx
sudo systemctl start nginx
}
```

And now let's configure NGINX!

### 3.3. Configure NGINX

On CentOS, NGINX stores: 
* the `nginx.conf` file under `/etc/nginx/nginx.conf`
* the logs under `/var/log/nginx/`

We're going to create an `nginx.conf` file that will **load balance** the worker nodes and the controller that we created in Step 1. 

Create a `nginx.conf` file under `/etc/nginx/nginx.conf` containing the following: 

```
http {

    upstream kubeingress {
        server 10.240.0.20:32071;
        server 10.240.0.21:32071;
        server 10.240.0.11:32071;
    }

    server {
        location / {
            proxy_pass http://kubeingress/;
        }
    }
}
events {}
```

That will balance the load across the 3 IP address (subnet) of the 2 worker nodes and the controller node of the K8S cluster.

**Make sure to run this**, to allow NGINX to make HTTP calls to the Ingress:

```
setsebool -P httpd_can_network_connect 1
```

Now to apply the new configuration: 

```
sudo nginx -s reload
```

### 3.4. Test the whole thing

Now, from anywhere on the Internet, you'll be able to curl into your Kubernetes `kapi-service`:

```
curl http://<external IP address of the proxy-0 VM>/api/base/status
```Â 
Result: 
```
{"status":"running","msg":"All is fine man! I am up and running"}
```


And you're done! :) 